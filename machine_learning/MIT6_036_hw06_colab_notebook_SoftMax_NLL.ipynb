{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "1vjtYChCxpJo"
      },
      "cell_type": "markdown",
      "source": [
        "# MIT 6.036 Spring 2019: Homework 6\n",
        "\n",
        "This homework does not include provided Python code. Instead, we encourage you to write your own code to help you answer some of these problems, and/or test and debug the code components we do ask for. All of the problems should be simple enough that hand calculation should be possible, but it may be convenient to write some short programs to explore the neural networks, particularly for problem 2.\n"
      ]
    },
    {
      "metadata": {
        "id": "Q0278eUVx91Z"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def SM(z):\n",
        "    # Compute the softmax of vector z\n",
        "    exp_z = np.exp(z)\n",
        "    return exp_z / np.sum(exp_z)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nhKmop0MBh9R"
      },
      "cell_type": "markdown",
      "source": [
        " **Problem 2A**"
      ]
    },
    {
      "metadata": {
        "id": "g0093Y5sBitt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d148ff9e-7e26-4d3a-d2ff-e180de8441a5"
      },
      "cell_type": "code",
      "source": [
        "z = np.array([[-1, 0, 1]]).T\n",
        "# Apply the softmax function\n",
        "a = SM(z)\n",
        "\n",
        "# Print the resulting probability distribution\n",
        "print(a)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.09003057]\n",
            " [0.24472847]\n",
            " [0.66524096]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 2B"
      ],
      "metadata": {
        "id": "zqmVqZzGnm-1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def nll(a, y):\n",
        "    # Compute NLL directly without clipping\n",
        "    return -np.sum(y * np.log(a) + (1 - y) * np.log(1 - a))\n",
        "\n",
        "# Given arrays\n",
        "a = np.array([[0.3], [0.5], [0.2]])  # (3, 1)\n",
        "y = np.array([[0], [0], [1]])        # (3, 1)\n",
        "\n",
        "# Compute and print NLL\n",
        "nll_value = nll(a, y)\n",
        "print(nll_value)\n",
        "\n",
        "# Print result with natural log value\n",
        "print(f\"NLL: {nll_value}\")\n",
        "print(f\"-log(0.2) = {np.log(1 / 0.2)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oesg5qW-xg3c",
        "outputId": "b7d18d55-b107-44da-8652-c4e7d2441c38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.6592600369327783\n",
            "NLL: 2.6592600369327783\n",
            "-log(0.2) = 1.6094379124341003\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "4ZigPSFHBOWy"
      },
      "cell_type": "markdown",
      "source": [
        "**Problem 2.C-F**"
      ]
    },
    {
      "metadata": {
        "id": "McPjfdEPBSNs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7da943ba-f813-4a16-c8ad-8b6c37c4d1b7"
      },
      "cell_type": "code",
      "source": [
        "#w = np.array([[1, -1, -2], [-1, 2, 1]])\n",
        "#x = np.array([[1], [1]])\n",
        "#y = np.array([[0, 1, 0]]).T\n",
        "# your code here\n",
        "import numpy as np\n",
        "\n",
        "def softmax(z):\n",
        "    exp_z = np.exp(z)\n",
        "    return exp_z / np.sum(exp_z)\n",
        "\n",
        "# Weight matrix\n",
        "w = np.array([[1, -1, -2], [-1, 2, 1]])\n",
        "\n",
        "# Input vector x\n",
        "x = np.array([[1], [1]])\n",
        "\n",
        "# Target output y\n",
        "y = np.array([[0, 1, 0]]).T\n",
        "\n",
        "# Compute pre-activation values z^L\n",
        "z = np.dot(w.T, x)\n",
        "\n",
        "# Compute softmax probabilities a^L\n",
        "a = softmax(z)\n",
        "\n",
        "# Compute the gradient of the weights with respect to the NLL\n",
        "gradient = np.dot(x, (a - y).T)\n",
        "\n",
        "# Print the gradient\n",
        "print(\"Gradient of the weights with respect to NLL:\")\n",
        "print(gradient)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient of the weights with respect to NLL:\n",
            "[[ 0.24472847 -0.33475904  0.09003057]\n",
            " [ 0.24472847 -0.33475904  0.09003057]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Predicting probability of class for x  ***"
      ],
      "metadata": {
        "id": "upZ6-Fg0Bffh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def softmax(z):\n",
        "    exp_z = np.exp(z)\n",
        "    return exp_z / np.sum(exp_z)\n",
        "\n",
        "# Initial weight matrix\n",
        "w = np.array([[1, -1, -2], [-1, 2, 1]])  # Shape (2, 3)\n",
        "\n",
        "# Input vector x\n",
        "x = np.array([[1], [1]])  # Shape (2, 1)\n",
        "\n",
        "# Target output y\n",
        "y = np.array([[0, 1, 0]]).T  # Shape (3, 1)\n",
        "\n",
        "# Compute pre-activation values z^L before gradient update\n",
        "z = np.dot(w.T, x)  # Shape (3, 1)\n",
        "\n",
        "# Compute softmax probabilities a^L before gradient update\n",
        "a = softmax(z)\n",
        "\n",
        "# Print probabilities before the gradient update\n",
        "print(\"Probabilities before gradient update:\")\n",
        "print(a)\n",
        "\n",
        "# Determine the predicted class before the gradient update\n",
        "predicted_class_before = np.argmax(a) + 1\n",
        "print(f\"Predicted class before gradient update: {predicted_class_before}\")\n",
        "\n",
        "# Compute the gradient of the weights with respect to NLL\n",
        "gradient = np.dot(x, (a - y).T)  # Shape (2, 3)\n",
        "\n",
        "# Update weights (assume a small learning rate, e.g., lr = 0.1)\n",
        "learning_rate = 0.1\n",
        "w_new = w - learning_rate * gradient  # No transpose, gradient is (2, 3)\n",
        "\n",
        "# Compute pre-activation values z^L after gradient update\n",
        "z_new = np.dot(w_new.T, x)  # Shape (3, 1)\n",
        "\n",
        "# Compute softmax probabilities a^L after gradient update\n",
        "a_new = softmax(z_new)\n",
        "\n",
        "# Print probabilities after the gradient update\n",
        "print(\"Probabilities after gradient update:\")\n",
        "print(a_new)\n",
        "\n",
        "# Determine the predicted class after the gradient update\n",
        "predicted_class_after = np.argmax(a_new) + 1\n",
        "print(f\"Predicted class after gradient update: {predicted_class_after}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42R2Ok6SBXKD",
        "outputId": "d2795ec5-d26a-4635-ad19-725facdca676"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Probabilities before gradient update:\n",
            "[[0.24472847]\n",
            " [0.66524096]\n",
            " [0.09003057]]\n",
            "Predicted class before gradient update: 2\n",
            "Probabilities after gradient update:\n",
            "[[0.22564471]\n",
            " [0.6887368 ]\n",
            " [0.08561849]]\n",
            "Predicted class after gradient update: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using step size 0.5"
      ],
      "metadata": {
        "id": "vkD1JOJiD0VN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import numpy as np\n",
        "\n",
        "#def softmax(z):\n",
        "#    exp_z = np.exp(z)\n",
        "#    return exp_z / np.sum(exp_z)\n",
        "\n",
        "# Initial weight matrix\n",
        "w = np.array([[1, -1, -2], [-1, 2, 1]])  # Shape (2, 3)\n",
        "\n",
        "# Input vector x\n",
        "x = np.array([[1], [1]])  # Shape (2, 1)\n",
        "\n",
        "# Target output y\n",
        "y = np.array([[0, 1, 0]]).T  # Shape (3, 1)\n",
        "\n",
        "# Compute pre-activation values z^L before gradient update\n",
        "z = np.dot(w.T, x)  # Shape (3, 1)\n",
        "\n",
        "# Compute softmax probabilities a^L before gradient update\n",
        "a = softmax(z)\n",
        "\n",
        "# Print probabilities before the gradient update\n",
        "print(\"Probabilities before gradient update:\")\n",
        "print(a)\n",
        "\n",
        "# Determine the predicted class before the gradient update\n",
        "predicted_class_before = np.argmax(a) + 1\n",
        "print(f\"Predicted class before gradient update: {predicted_class_before}\")\n",
        "\n",
        "# Compute the gradient of the weights with respect to NLL\n",
        "gradient = np.dot(x, (a - y).T)  # Shape (2, 3)\n",
        "\n",
        "# Update weights with step size 0.5 (learning rate = 0.5)\n",
        "learning_rate = 0.5\n",
        "w_new = w - learning_rate * gradient  # Shape (2, 3)\n",
        "\n",
        "# Print the updated weight matrix\n",
        "print(\"Updated weight matrix WL after one gradient update step with step size 0.5:\")\n",
        "print(w_new)\n",
        "\n",
        "# Compute pre-activation values z^L after gradient update\n",
        "z_new = np.dot(w_new.T, x)  # Shape (3, 1)\n",
        "\n",
        "# Compute softmax probabilities a^L after gradient update\n",
        "a_new = softmax(z_new)\n",
        "\n",
        "# Print probabilities after the gradient update\n",
        "print(\"Probabilities after gradient update:\")\n",
        "print(a_new)\n",
        "\n",
        "# Determine the predicted class after the gradient update\n",
        "predicted_class_after = np.argmax(a_new) + 1\n",
        "print(f\"Predicted class after gradient update: {predicted_class_after}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PAwwkOfxD6CS",
        "outputId": "ca0f2919-5522-4250-b902-03c52ac32cb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Probabilities before gradient update:\n",
            "[[0.24472847]\n",
            " [0.66524096]\n",
            " [0.09003057]]\n",
            "Predicted class before gradient update: 2\n",
            "Updated weight matrix WL after one gradient update step with step size 0.5:\n",
            "[[ 0.87763576 -0.83262048 -2.04501529]\n",
            " [-1.12236424  2.16737952  0.95498471]]\n",
            "Probabilities after gradient update:\n",
            "[[0.15918761]\n",
            " [0.77245284]\n",
            " [0.06835955]]\n",
            "Predicted class after gradient update: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Find the predicted probability that x is in class 1 using the updated weight matrix***"
      ],
      "metadata": {
        "id": "RBQuwcxmE7Ha"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def softmax(z):\n",
        "    exp_z = np.exp(z)\n",
        "    return exp_z / np.sum(exp_z)\n",
        "\n",
        "# Updated weight matrix WL after the gradient update\n",
        "new_w = np.array([[ 0.87763576, -0.83262048, -2.04501529],\n",
        "                  [-1.12236424,  2.16737952,  0.95498471]])  # Shape (2, 3)\n",
        "\n",
        "# Input vector x\n",
        "x = np.array([[1], [1]])  # Shape (2, 1)\n",
        "\n",
        "# Compute the pre-activation values z^L using the updated weight matrix\n",
        "new_z = np.dot(new_w.T, x)  # Shape (3, 1)\n",
        "\n",
        "# Compute the softmax probabilities a^L\n",
        "new_a = softmax(new_z)\n",
        "\n",
        "# Print the predicted probabilities\n",
        "print(\"Predicted probabilities (in order of classes 2, 1, boundary):\", new_a)\n",
        "\n",
        "# Mapping probabilities to classes based on the guide's hint\n",
        "prob_class_2 = new_a[0, 0]  # First element for class 2\n",
        "prob_class_1 = new_a[1, 0]  # Second element for class 1\n",
        "prob_boundary = new_a[2, 0]  # Third element for the boundary class\n",
        "\n",
        "print(f\"Predicted probability for class 1: {prob_class_1}\")\n",
        "print(f\"Predicted probability for class 2: {prob_class_2}\")\n",
        "print(f\"Predicted probability for boundary: {prob_boundary}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ZTUHJrKFAkN",
        "outputId": "1e56432f-eb8c-4baa-cfe5-01a3b5964b96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted probabilities (in order of classes 2, 1, boundary): [[0.15918761]\n",
            " [0.77245284]\n",
            " [0.06835955]]\n",
            "Predicted probability for class 1: 0.7724528373387168\n",
            "Predicted probability for class 2: 0.15918760859452763\n",
            "Predicted probability for boundary: 0.06835955406675555\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "6nefgHcxHgLe"
      },
      "cell_type": "markdown",
      "source": [
        "**Problem 3**"
      ]
    },
    {
      "metadata": {
        "id": "hFMHmf8NHgY5"
      },
      "cell_type": "code",
      "source": [
        "# layer 1 weights\n",
        "w_1 = np.array([[1, 0, -1, 0], [0, 1, 0, -1]])\n",
        "w_1_bias = np.array([[-1, -1, -1, -1]]).T\n",
        "# layer 2 weights\n",
        "w_2 = np.array([[1, -1], [1, -1], [1, -1], [1, -1]])\n",
        "w_2_bias = np.array([[0, 2]]).T\n",
        "\n",
        "# your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5Qtv3VR2AA_m",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "798b1f01-fbe2-4fc4-9482-f979de216f98"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "T  = np.matrix([[0.0 , 0.1 , 0.9 , 0.0],\n",
        "[0.9 , 0.1 , 0.0 , 0.0],\n",
        "[0.0 , 0.0 , 0.1 , 0.9],\n",
        "[0.9 , 0.0 , 0.0 , 0.1]])\n",
        "g = 0.9\n",
        "r = np.matrix([0, 1., 0., 2.]).reshape(4, 1)\n",
        "\n",
        "print(np.linalg.solve(np.eye(4) - g * T, r))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[6.05288295]\n",
            " [6.48663207]\n",
            " [6.7519581 ]\n",
            " [7.58553317]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}