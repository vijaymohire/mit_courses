{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q58cS9antfCw"
      },
      "source": [
        "#MIT 6.036 Spring 2019: Homework 10#\n",
        "\n",
        "This colab notebook provides code and a framework for questions 2, 3, and 4 from [homework 10](https://openlearninglibrary.mit.edu/courses/course-v1:MITx+6.036+1T2019/courseware/Week10/week10_homework/).  You can work out your solutions here, then submit your results back on the homework page when ready.\n",
        "\n",
        "## <section>**Setup**</section>\n",
        "\n",
        "First, download the code distribution for this homework that contains test cases and helper functions.\n",
        "\n",
        "Run the next code block to download and import the code for this lab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUEtSZRdtmI2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf5da79d-49ab-48e8-f763-5210341ef968"
      },
      "source": [
        "#!rm -rf code_for_hw10* __MACOSX data .DS_Store\n",
        "\n",
        "#!wget --quiet https://introml_oll.odl.mit.edu/cat-soop/_static/6.036/homework/hw10/code_for_hw10.zip\n",
        "#!unzip code_for_hw10.zip\n",
        "#!mv code_for_hw10/* .\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  code_for_hw10.zip\n",
            "   creating: code_for_hw10/\n",
            "  inflating: code_for_hw10/code_for_hw10.py  \n",
            "  inflating: code_for_hw10/dist.py   \n",
            "  inflating: code_for_hw10/mdp10.py  \n",
            "  inflating: code_for_hw10/util.py   \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m4vjJJINRYhY",
        "outputId": "5ca34aad-f79b-46b6-bb41-aeac0a2f9663"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''import zipfile\n",
        "import os\n",
        "\n",
        "# Define the path to your ZIP file and extraction folder, new files that have TF and Keras\n",
        "zip_file_path = '/content/drive/My Drive/code_for_hw10.zip'\n",
        "extract_folder = '/content/drive/My Drive/Colab Notebooks/'\n",
        "\n",
        "# Create the extraction folder if it doesn't exist\n",
        "os.makedirs(extract_folder, exist_ok=True)\n",
        "\n",
        "# Extract the ZIP file\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_folder)\n",
        "\n",
        "print(\"Extraction completed.\")'''\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N7ZvzpCSRbsj",
        "outputId": "9437e3d2-7060-4725-ee70-13f4435654da"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extraction completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf code_for_hw10*"
      ],
      "metadata": {
        "id": "4TSYMmTUpY1F"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Make new code_for_hw10 with TF based Keras imports and replace that  before importing to  HW 10 code below\n",
        "#manually Wget files and unzip locally and upload to related drive root folder so that no above issues are seen"
      ],
      "metadata": {
        "id": "5jAPpQLKZXm4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pdb\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import dist\n",
        "import util\n",
        "import pickle\n",
        "#import pdb\n",
        "import random\n",
        "import numpy as np\n",
        "from dist import uniform_dist, delta_dist, mixture_dist\n",
        "from util import argmax_with_val, argmax\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "class MDP:\n",
        "    # Needs the following attributes:\n",
        "    # states: list or set of states\n",
        "    # actions: list or set of actions\n",
        "    # discount_factor: real, greater than 0, less than or equal to 1\n",
        "    # start: optional instance of DDist, specifying initial state dist\n",
        "    #    if it's unspecified, we'll use a uniform over states\n",
        "    # These are functions:\n",
        "    # transition_model: function from (state, action) into DDist over next state\n",
        "    # reward_fn: function from (state, action) to real-valued reward\n",
        "\n",
        "    def __init__(self, states, actions, transition_model, reward_fn,\n",
        "                     discount_factor = 1.0, start_dist = None):\n",
        "        self.states = states\n",
        "        self.actions = actions\n",
        "        self.transition_model = transition_model\n",
        "        self.reward_fn = reward_fn\n",
        "        self.discount_factor = discount_factor\n",
        "        self.start = start_dist if start_dist else uniform_dist(states)\n",
        "\n",
        "    # Given a state, return True if the state should be considered to\n",
        "    # be terminal.  You can think of a terminal state as generating an\n",
        "    # infinite sequence of zero reward.\n",
        "    def terminal(self, s):\n",
        "        return False\n",
        "\n",
        "    # Randomly choose a state from the initial state distribution\n",
        "    def init_state(self):\n",
        "        return self.start.draw()\n",
        "\n",
        "    # Simulate a transition from state s, given action a.  Return\n",
        "    # reward for (s,a) and new state, drawn from transition.  If a\n",
        "    # terminal state is encountered, sample next state from initial\n",
        "    # state distribution\n",
        "    def sim_transition(self, s, a):\n",
        "        return (self.reward_fn(s, a),\n",
        "                self.init_state() if self.terminal(s) else\n",
        "                    self.transition_model(s, a).draw())\n",
        "\n",
        "    def state2vec(self, s):\n",
        "        '''\n",
        "        Return one-hot encoding of state s; used in neural network agent implementations\n",
        "        '''\n",
        "        v = np.zeros((1, len(self.states)))\n",
        "        v[0,self.states.index(s)] = 1.\n",
        "        return v\n",
        "\n",
        "# Perform value iteration on an MDP, also given an instance of a q\n",
        "# function.  Terminate when the max-norm distance between two\n",
        "# successive value function estimates is less than eps.\n",
        "# interactive_fn is an optional function that takes the q function as\n",
        "# argument; if it is not None, it will be called once per iteration,\n",
        "# for visuzalization\n",
        "\n",
        "# The q function is typically an instance of TabularQ, implemented as a\n",
        "# dictionary mapping (s, a) pairs into Q values This must be\n",
        "# initialized before interactive_fn is called the first time.\n",
        "\n",
        "def value_iteration(mdp, q, eps = 0.01, max_iters = 1000):\n",
        "    # Your code here (COPY FROM HW9)\n",
        "    raise NotImplementedError('value_iteration')\n",
        "\n",
        "# Given a state, return the value of that state, with respect to the\n",
        "# current definition of the q function\n",
        "def value(q, s):\n",
        "    # Your code here (COPY FROM HW9)\n",
        "    raise NotImplementedError('value')\n",
        "\n",
        "# Given a state, return the action that is greedy with reespect to the\n",
        "# current definition of the q function\n",
        "def greedy(q, s):\n",
        "    # Your code here (COPY FROM HW9)\n",
        "    raise NotImplementedError('greedy')\n",
        "\n",
        "def epsilon_greedy(q, s, eps = 0.5):\n",
        "    if random.random() < eps:  # True with prob eps, random action\n",
        "        # Your code here (COPY FROM HW9)\n",
        "        raise NotImplementedError('epsilon_greedy')\n",
        "    else:\n",
        "        # Your code here (COPY FROM HW9)\n",
        "        raise NotImplementedError('epsilon_greedy')\n",
        "\n",
        "class TabularQ:\n",
        "    def __init__(self, states, actions):\n",
        "        self.actions = actions\n",
        "        self.states = states\n",
        "        self.q = dict([((s, a), 0.0) for s in states for a in actions])\n",
        "    def copy(self):\n",
        "        q_copy = TabularQ(self.states, self.actions)\n",
        "        q_copy.q.update(self.q)\n",
        "        return q_copy\n",
        "    def set(self, s, a, v):\n",
        "        self.q[(s,a)] = v\n",
        "    def get(self, s, a):\n",
        "        return self.q[(s,a)]\n",
        "    def update(self, data, lr):\n",
        "        # Your code here\n",
        "        raise NotImplementedError('TabularQ.update')\n",
        "\n",
        "def Q_learn(mdp, q, lr=.1, iters=100, eps = 0.5, interactive_fn=None):\n",
        "    # Your code here\n",
        "    raise NotImplementedError('Q_learn')\n",
        "    for i in range(iters):\n",
        "        # include this line in the iteration, where i is the iteration number\n",
        "        if interactive_fn: interactive_fn(q, i)\n",
        "    pass\n",
        "\n",
        "# Simulate an episode (sequence of transitions) of at most\n",
        "# episode_length, using policy function to select actions.  If we find\n",
        "# a terminal state, end the episode.  Return accumulated reward a list\n",
        "# of (s, a, r, s') where s' is None for transition from terminal state.\n",
        "# Also return an animation if draw=True.\n",
        "def sim_episode(mdp, episode_length, policy, draw=False):\n",
        "    episode = []\n",
        "    reward = 0\n",
        "    s = mdp.init_state()\n",
        "    all_states = [s]\n",
        "    for i in range(int(episode_length)):\n",
        "        a = policy(s)\n",
        "        (r, s_prime) = mdp.sim_transition(s, a)\n",
        "        reward += r\n",
        "        if mdp.terminal(s):\n",
        "            episode.append((s, a, r, None))\n",
        "            break\n",
        "        episode.append((s, a, r, s_prime))\n",
        "        if draw:\n",
        "            mdp.draw_state(s)\n",
        "        s = s_prime\n",
        "        all_states.append(s)\n",
        "    animation = animate(all_states, mdp.n, episode_length) if draw else None\n",
        "    return reward, episode, animation\n",
        "\n",
        "# Create a matplotlib animation from all states of the MDP that\n",
        "# can be played both in colab and in local versions.\n",
        "def animate(states, n, ep_length):\n",
        "    try:\n",
        "        from matplotlib import animation, rc\n",
        "        import matplotlib.pyplot as plt\n",
        "        from google.colab import widgets\n",
        "\n",
        "        plt.ion()\n",
        "        plt.figure(facecolor=\"white\")\n",
        "        fig, ax = plt.subplots()\n",
        "        plt.close()\n",
        "\n",
        "        def animate(i):\n",
        "            if states[i % len(states)] == None or states[i % len(states)] == 'over':\n",
        "                return\n",
        "            ((br, bc), (brv, bcv), pp, pv) = states[i % len(states)]\n",
        "            im = np.zeros((n, n+1))\n",
        "            im[br, bc] = -1\n",
        "            im[pp, n] = 1\n",
        "            ax.cla()\n",
        "            ims = ax.imshow(im, interpolation = 'none',\n",
        "                        cmap = 'viridis',\n",
        "                        extent = [-0.5, n+0.5,\n",
        "                                    -0.5, n-0.5],\n",
        "                        animated = True)\n",
        "            ims.set_clim(-1, 1)\n",
        "        rc('animation', html='jshtml')\n",
        "        anim = animation.FuncAnimation(fig, animate, frames=ep_length, interval=100)\n",
        "        return anim\n",
        "    except:\n",
        "        # we are not in colab, so the typical animation should work\n",
        "        return None\n",
        "\n",
        "# Return average reward for n_episodes of length episode_length\n",
        "# while following policy (a function of state) to choose actions.\n",
        "def evaluate(mdp, n_episodes, episode_length, policy):\n",
        "    score = 0\n",
        "    length = 0\n",
        "    for i in range(n_episodes):\n",
        "        # Accumulate the episode rewards\n",
        "        r, e, _ = sim_episode(mdp, episode_length, policy)\n",
        "        score += r\n",
        "        length += len(e)\n",
        "        # print('    ', r, len(e))\n",
        "    return score/n_episodes, length/n_episodes\n",
        "\n",
        "def Q_learn_batch(mdp, q, lr=.1, iters=100, eps=0.5,\n",
        "                  episode_length=10, n_episodes=2,\n",
        "                  interactive_fn=None):\n",
        "    # Your code here\n",
        "    raise NotImplementedError('Q_learn_batch')\n",
        "    for i in range(iters):\n",
        "        # include this line in the iteration, where i is the iteration number\n",
        "        if interactive_fn: interactive_fn(q, i)\n",
        "    pass\n",
        "\n",
        "def make_nn(state_dim, num_hidden_layers, num_units):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(num_units, input_dim = state_dim, activation='relu'))\n",
        "    for i in range(num_hidden_layers-1):\n",
        "        model.add(Dense(num_units, activation='relu'))\n",
        "    model.add(Dense(1, activation='linear'))\n",
        "    model.compile(loss='mse', optimizer=Adam())\n",
        "    return model\n",
        "\n",
        "class NNQ:\n",
        "    def __init__(self, states, actions, state2vec, num_layers, num_units, epochs=1):\n",
        "        self.actions = actions\n",
        "        self.states = states\n",
        "        self.state2vec = state2vec\n",
        "        self.epochs = epochs\n",
        "        self.models = None              # Your code here\n",
        "        if self.models is None: raise NotImplementedError('NNQ.models')\n",
        "    def get(self, s, a):\n",
        "        # Your code here\n",
        "        raise NotImplementedError('NNQ.get')\n",
        "    def update(self, data, lr):\n",
        "        # Your code here\n",
        "        raise NotImplementedError('NNQ.update')\n",
        "\n",
        "\n",
        "class No_Exit(MDP):\n",
        "    # Like breakout or pong, but one player, no walls to break out, no\n",
        "    # way to win You can move paddle vertically up or down or stay\n",
        "    actions = (+1, 0, -1)\n",
        "\n",
        "    def __init__(self, field_size, ball_speed = 1, random_start = True):\n",
        "        # image space is n by n\n",
        "        self.q = None\n",
        "        self.n = field_size\n",
        "        h = self.n * ball_speed\n",
        "        self.discount_factor = (h - 1.0) / h\n",
        "        self.ball_speed = ball_speed\n",
        "        # state space is: ball position and velocity, paddle position\n",
        "        # and velocity\n",
        "        # - ball position is n by n\n",
        "        # - ball velocity is one of (-1, -1), (-1, 1), (0, -1), (0, 1),\n",
        "        #                          (1, -1), (1, 1)\n",
        "        # - paddle position is n; this is location of bottom of paddle,\n",
        "        #    can stick \"up\" out of the screen\n",
        "        # - paddle velocity is one of 1, 0, -1\n",
        "        self.states = [((br, bc), (brv, bcv), pp, pv) for \\\n",
        "                         br in range(self.n) for\n",
        "                         bc in range(self.n) for\n",
        "                         brv in (-1, 0, 1) for\n",
        "                         bcv in (-1, 1) for\n",
        "                         pp in range(self.n) for\n",
        "                         pv in (-1, 0, 1)]\n",
        "        self.states.append('over')\n",
        "        self.start = dist.uniform_dist([((br, 0), (0, 1), 0, 0) \\\n",
        "                                        for br in range(self.n)]) \\\n",
        "                if random_start else  \\\n",
        "                dist.delta_dist(((int(self.n/2), 0), (0, 1), 0, 0))\n",
        "\n",
        "    ax = None\n",
        "    ims = None\n",
        "    # Updating values in google colab\n",
        "    try:\n",
        "        from google.colab import widgets\n",
        "        IS_COLAB = True\n",
        "        grid = widgets.Grid(1, 10, header_row=False, header_column=False)\n",
        "        parity = 0\n",
        "    except:\n",
        "        IS_COLAB = False\n",
        "        grid = None\n",
        "\n",
        "    def draw_state(self, state = None, pause = False):\n",
        "        def _update(self, state, pause):\n",
        "            if self.ax is None or self.IS_COLAB:\n",
        "                plt.ion()\n",
        "                plt.figure(facecolor=\"white\")\n",
        "                self.ax = plt.subplot()\n",
        "\n",
        "            if state is None: state = self.state\n",
        "            ((br, bc), (brv, bcv), pp, pv) = state\n",
        "            im = np.zeros((self.n, self.n+1))\n",
        "            im[br, bc] = -1\n",
        "            im[pp, self.n] = 1\n",
        "            self.ax.cla()\n",
        "            self.ims = self.ax.imshow(im, interpolation = 'none',\n",
        "                                    cmap = 'viridis',\n",
        "                                    extent = [-0.5, self.n+0.5,\n",
        "                                                -0.5, self.n-0.5],\n",
        "                                    animated = True)\n",
        "            self.ims.set_clim(-1, 1)\n",
        "            plt.pause(0.0001)\n",
        "            if pause: input('go?')\n",
        "            else: plt.pause(0.1 if self.IS_COLAB else 0.01)\n",
        "\n",
        "        if self.IS_COLAB:\n",
        "            with self.grid.output_to(0, (self.parity % 10)):\n",
        "                # _update(self, state, pause)\n",
        "                self.grid.clear_cell(0, (self.parity + 1) % 10)\n",
        "                self.parity =  (self.parity + 9) % 10\n",
        "        else:\n",
        "            _update(self, state, pause)\n",
        "\n",
        "    def state2vec(self, s):\n",
        "        if s == 'over':\n",
        "            return np.array([[0, 0, 0, 0, 0, 0, 1]])\n",
        "        ((br, bc), (brv, bcv), pp, pv) = s\n",
        "        return np.array([[br, bc, brv, bcv, pp, pv, 0]])\n",
        "\n",
        "    def terminal(self, state):\n",
        "        return state == 'over'\n",
        "\n",
        "    def reward_fn(self, s, a):\n",
        "        return 0 if s == 'over' else 1\n",
        "\n",
        "    def transition_model(self, s, a, p = 0.4):\n",
        "        # Only randomness is in brv and brc after a bounce\n",
        "        # 1- prob of negating nominal velocity\n",
        "        if s == 'over':\n",
        "            return dist.delta_dist('over')\n",
        "        # Current state\n",
        "        ((br, bc), (brv, bcv), pp, pv) = s\n",
        "        # Nominal next ball state\n",
        "        new_br = br + self.ball_speed*brv; new_brv = brv\n",
        "        new_bc = bc + self.ball_speed*bcv; new_bcv = bcv\n",
        "        # nominal paddle state, a is action (-1, 0, 1)\n",
        "        new_pp = max(0, min(self.n-1, pp + a))\n",
        "        new_pv = a\n",
        "        new_s = None\n",
        "        hit_r = hit_c = False\n",
        "        # bottom, top contacts\n",
        "        if new_br < 0:\n",
        "            new_br = 0; new_brv = 1; hit_r = True\n",
        "        elif new_br >= self.n:\n",
        "            new_br = self.n - 1; new_brv = -1; hit_r = True\n",
        "        # back, front contacts\n",
        "        if new_bc < 0:                  # back bounce\n",
        "            new_bc = 0; new_bcv = 1; hit_c = True\n",
        "        elif new_bc >= self.n:\n",
        "            if self.paddle_hit(pp, new_pp, br, bc, new_br, new_bc):\n",
        "                new_bc = self.n-1; new_bcv = -1; hit_c = True\n",
        "            else:\n",
        "                return dist.delta_dist('over')\n",
        "\n",
        "        new_s = ((new_br, new_bc), (new_brv, new_bcv), new_pp, new_pv)\n",
        "        if ((not hit_c) and (not hit_r)):\n",
        "            return dist.delta_dist(new_s)\n",
        "        elif hit_c:                     # also hit_c and hit_r\n",
        "            if abs(new_brv) > 0:\n",
        "                return dist.DDist({new_s: p,\n",
        "                                   ((new_br, new_bc), (-new_brv, new_bcv), new_pp, new_pv) : 1-p})\n",
        "            else:\n",
        "                return dist.DDist({new_s: p,\n",
        "                                   ((new_br, new_bc), (-1, new_bcv), new_pp, new_pv) : 0.5*(1-p),\n",
        "                                   ((new_br, new_bc), (1, new_bcv), new_pp, new_pv) : 0.5*(1-p)})\n",
        "        elif hit_r:\n",
        "            return dist.DDist({new_s: p,\n",
        "                               ((new_br, new_bc), (new_brv, -new_bcv), new_pp, new_pv) : 1-p})\n",
        "\n",
        "\n",
        "    def paddle_hit(self, pp, new_pp, br, bc, new_br, new_bc):\n",
        "        # Being generous to paddle, any overlap in row\n",
        "        prset = set(range(pp, pp+2)).union(set(range(new_pp, new_pp+2)))\n",
        "        brset = set([br, br+1, new_br, new_br+1])\n",
        "        return len(prset.intersection(brset)) >= 2\n",
        "\n",
        "##############################\n",
        "# Display\n",
        "##############################\n",
        "def tidy_plot(xmin, xmax, ymin, ymax, center = False, title = None,\n",
        "                 xlabel = None, ylabel = None):\n",
        "    # plt.ion()\n",
        "    plt.figure(facecolor=\"white\")\n",
        "    ax = plt.subplot()\n",
        "    if center:\n",
        "        ax.spines['left'].set_position('zero')\n",
        "        ax.spines['right'].set_color('none')\n",
        "        ax.spines['bottom'].set_position('zero')\n",
        "        ax.spines['top'].set_color('none')\n",
        "        ax.spines['left'].set_smart_bounds(True)\n",
        "        ax.spines['bottom'].set_smart_bounds(True)\n",
        "        ax.xaxis.set_ticks_position('bottom')\n",
        "        ax.yaxis.set_ticks_position('left')\n",
        "    else:\n",
        "        ax.spines[\"top\"].set_visible(False)\n",
        "        ax.spines[\"right\"].set_visible(False)\n",
        "        ax.get_xaxis().tick_bottom()\n",
        "        ax.get_yaxis().tick_left()\n",
        "    eps = .05\n",
        "    plt.xlim(xmin-eps, xmax+eps)\n",
        "    plt.ylim(ymin-eps, ymax+eps)\n",
        "    if title: ax.set_title(title)\n",
        "    if xlabel: ax.set_xlabel(xlabel)\n",
        "    if ylabel: ax.set_ylabel(ylabel)\n",
        "    return ax\n",
        "\n",
        "def plot_points(x, y, ax = None, clear = False,\n",
        "                  xmin = None, xmax = None, ymin = None, ymax = None,\n",
        "                  style = 'or-'):\n",
        "\n",
        "    if ax is None:\n",
        "        if xmin == None: xmin = np.min(x) - 0.5\n",
        "        if xmax == None: xmax = np.max(x) + 0.5\n",
        "        if ymin == None: ymin = np.min(y) - 0.5\n",
        "        if ymax == None: ymax = np.max(y) + 0.5\n",
        "        ax = tidy_plot(xmin, xmax, ymin, ymax)\n",
        "\n",
        "        x_range = xmax - xmin; y_range = ymax - ymin\n",
        "        if .1 < x_range / y_range < 10:\n",
        "            plt.axis('equal')\n",
        "        xlim, ylim = ax.get_xlim(), ax.get_ylim()\n",
        "    elif clear:\n",
        "        xlim, ylim = ax.get_xlim(), ax.get_ylim()\n",
        "        ax.clear()\n",
        "    else:\n",
        "        xlim, ylim = ax.get_xlim(), ax.get_ylim()\n",
        "    ax.plot(x, y, style, markeredgewidth=0.0)\n",
        "    # Seems to occasionally mess up the limits\n",
        "    # ax.set_xlim(xlim); ax.set_ylim(ylim)\n",
        "    ax.grid(True, which='both')\n",
        "    plt.show()\n",
        "    return ax\n",
        "\n",
        "import functools\n",
        "def toHex(s):\n",
        "    lst = []\n",
        "    for ch in s:\n",
        "        hv = hex(ord(ch)).replace('0x', '')\n",
        "        if len(hv) == 1:\n",
        "            hv = '0'+hv\n",
        "        lst.append(hv)\n",
        "\n",
        "    return functools.reduce(lambda x,y:x+y, lst)\n",
        "\n",
        "##############################\n",
        "\n",
        "def test_learn_play(d = 6, num_layers = 2, num_units = 100,\n",
        "                    eps = 0.5, iters = 10000, draw=False,\n",
        "                    tabular = True, batch=False, batch_epochs=10,\n",
        "                    num_episodes = 10, episode_length = 100):\n",
        "    iters_per_value = 1 if iters <= 10 else int(iters / 10.0)\n",
        "    scores = []\n",
        "    def interact(q, iter=0):\n",
        "        if iter % iters_per_value == 0:\n",
        "            scores.append((iter, evaluate(game, num_episodes, episode_length,\n",
        "                                          lambda s: greedy(q, s))[0]))\n",
        "            print('score', scores[-1], flush=True)\n",
        "    game = No_Exit(d)\n",
        "    if tabular:\n",
        "        q = TabularQ(game.states, game.actions)\n",
        "    else:\n",
        "        q = NNQ(game.states, game.actions, game.state2vec, num_layers, num_units,\n",
        "                epochs=batch_epochs if batch else 1)\n",
        "    if batch:\n",
        "        qf = Q_learn_batch(game, q, iters=iters, episode_length = 100, n_episodes=10,\n",
        "                           interactive_fn=interact)\n",
        "    else:\n",
        "        qf = Q_learn(game, q, iters=iters, interactive_fn=interact)\n",
        "    if scores:\n",
        "        print('String to upload (incude quotes): \"%s\"'%toHex(pickle.dumps([tabular, batch, scores], 0).decode()))\n",
        "        # Plot learning curve\n",
        "        plot_points(np.array([s[0] for s in scores]),\n",
        "                    np.array([s[1] for s in scores]))\n",
        "    for i in range(num_episodes):\n",
        "        reward, _, animation = sim_episode(game, (episode_length if d > 5 else episode_length/2),\n",
        "                                lambda s: greedy(qf, s), draw=draw)\n",
        "        print('Reward', reward)\n",
        "    return animation\n",
        "\n",
        "\n",
        "def test_solve_play(d = 6, draw=False,\n",
        "                    num_episodes = 10, episode_length = 100):\n",
        "    game = No_Exit(d)\n",
        "    qf = value_iteration(game , TabularQ(game.states, game.actions))\n",
        "    for i in range(num_episodes):\n",
        "        reward, _, animation = sim_episode(game, (episode_length if d > 5 else episode_length/2),\n",
        "                                lambda s: greedy(qf, s), draw=draw)\n",
        "        print('Reward', reward)\n",
        "    return animation\n",
        "\n",
        "##########   Test cases\n",
        "\n",
        "# Value Iteration\n",
        "# test_solve_play()\n",
        "# Tabular Q-learn\n",
        "# test_learn_play(iters=100000, tabular=True, batch=False)\n",
        "# Tabular Batch Q-learn\n",
        "# test_learn_play(iters=10, tabular=True, batch=True) # Check: why do we want fewer iterations here?\n",
        "# NN Q-learn\n",
        "# test_learn_play(iters=100000, tabular=False, batch=False)\n",
        "# NN Batch Q-learn (Fitted Q-learn)\n",
        "# test_learn_play(iters=10, tabular=False, batch=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 21
        },
        "id": "2Ori_JD7qJk7",
        "outputId": "e7a763b4-4085-4757-d5f1-d9ecaab01831"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "       table#id2, #id2 > tbody > tr > th, #id2 > tbody > tr > td {\n",
              "         border: 1px solid lightgray;\n",
              "         border-collapse:collapse;\n",
              "         \n",
              "        }</style>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table id=id2><tr><td id=id2-0-0></td><td id=id2-0-1></td><td id=id2-0-2></td><td id=id2-0-3></td><td id=id2-0-4></td><td id=id2-0-5></td><td id=id2-0-6></td><td id=id2-0-7></td><td id=id2-0-8></td><td id=id2-0-9></td></tr></table>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#import code_for_hw10 as code_for_hw10\n",
        "#import mdp10 as mdp\n",
        "\n",
        "import numpy as np\n",
        "import math as m\n",
        "import random\n",
        "\n",
        "import pdb\n",
        "from dist import uniform_dist, delta_dist, mixture_dist, DDist\n",
        "from util import argmax_with_val, argmax\n",
        "#from keras.models import Sequential\n",
        "#from keras.layers.core import Dense\n",
        "#from keras.optimizers import Adam\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "import importlib"
      ],
      "metadata": {
        "id": "fcA8CLOxROiC"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Zhptv005XBN"
      },
      "source": [
        "# 2) Implement Q-Learning\n",
        "\n",
        "We'll work up to implementing the Q-learning algorithm by extending our code from HW9. In the next block, please copy and paste your implementations of the following functions from HW9."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0cu9UMTm2l8x"
      },
      "source": [
        "def value_iteration(mdp, q, eps = 0.01, max_iters = 1000):\n",
        "    # Your code here (COPY FROM HW9)\n",
        "    for i in range(max_iters):\n",
        "        max_value = 0\n",
        "        max_a, max_s = None,None\n",
        "        new_q = q.copy()\n",
        "        for pairs in q.q:\n",
        "            s,a = pairs[0], pairs[1]\n",
        "            v = mdp.reward_fn(s,a)\n",
        "            v += mdp.discount_factor * mdp.transition_model(s,a).expectation(lambda s: value(q,s))\n",
        "            new_q.set(s,a,v)\n",
        "            if v>max_value:\n",
        "                max_value = v\n",
        "                max_a,max_s = a,s\n",
        "        if new_q.q[(max_s,max_a)] - q.q[(max_s,max_a)]<eps:\n",
        "            return new_q\n",
        "        q = new_q.copy()\n",
        "\n",
        "    raise NotImplementedError('value_iteration')\n",
        "\n",
        "def value(q, s):\n",
        "    # Your code here (COPY FROM HW9)\n",
        "    q_star = None\n",
        "    for a in q.actions:\n",
        "        if q_star is None:\n",
        "            q_star = q.q[(s,a)]\n",
        "        elif q.q[(s,a)] > q_star:\n",
        "            q_star = q.q[(s,a)]\n",
        "    return q_star\n",
        "\n",
        "    raise NotImplementedError('value')\n",
        "\n",
        "def greedy(q, s):\n",
        "    # Your code here (COPY FROM HW9)\n",
        "    return argmax(q.actions, lambda a: q.get(s, a))\n",
        "\n",
        "    raise NotImplementedError('greedy')\n",
        "\n",
        "def epsilon_greedy(q, s, eps = 0.5):\n",
        "    if random.random() < eps:  # True with prob eps, random action\n",
        "        # Your code here (COPY FROM HW9)\n",
        "        return uniform_dist(q.actions).draw()\n",
        "        raise NotImplementedError('epsilon_greedy')\n",
        "    else:\n",
        "        # Your code here (COPY FROM HW9)\n",
        "        return action\n",
        "        raise NotImplementedError('epsilon_greedy')"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MyKQPeWk5zx1"
      },
      "source": [
        "Run the next code block to make sure what you need from HW9 is working."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fvz2c_Vs3JuN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c211bbcf-1bd4-4b38-d7e5-0bca34120e7b"
      },
      "source": [
        "MDP.value = value\n",
        "MDP.greedy = greedy\n",
        "MDP.epsilon_greedy = epsilon_greedy\n",
        "MDP.value_iteration = value_iteration\n",
        "\n",
        "#importlib.reload(code_for_hw10)\n",
        "\n",
        "# Test: Value Iteration\n",
        "#code_for_hw10.test_solve_play()\n",
        "\n",
        "test_solve_play()\n",
        "# Expected output:\n",
        "# '''\n",
        "# Reward 100\n",
        "# Reward 100\n",
        "# Reward 100\n",
        "# Reward 100\n",
        "# Reward 100\n",
        "# Reward 100\n",
        "# Reward 100\n",
        "# Reward 100\n",
        "# Reward 100\n",
        "# Reward 100\n",
        "# '''"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reward 100\n",
            "Reward 100\n",
            "Reward 100\n",
            "Reward 100\n",
            "Reward 100\n",
            "Reward 100\n",
            "Reward 100\n",
            "Reward 100\n",
            "Reward 100\n",
            "Reward 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCaiqNqB6D2-"
      },
      "source": [
        "## 2.1) Q update\n",
        "\n",
        "First, we'll extend our implementation of the TabularQ class in HW 9 (Problem 5) to incorporate the crucial operation of Q-learning, which is to update the Q value for a given `(s, a)` entry and move it part of the way towards a \"target\" value *t*.\n",
        "\n",
        "> *Q(s,a) ← (1−α)Q(s,a) + αt*\n",
        "\n",
        "Note that this can also be written as:\n",
        "\n",
        "> *Q(s,a)← Q(s,a)+α(t−Q(s,a))*\n",
        "\n",
        "That is, move a small (*α*) step towards t."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RL0934247JNK"
      },
      "source": [
        "Let's define a new method for `TabularQ` that implements this, in a batched form. We will be given a list of `(s, a, t)` triples and have to do all the updates. Note that update is a method of the `TabularQ` class, so you can access the other methods and attributes.\n",
        "\n",
        "* `data` is a list of `(s, a, t)` tuples.\n",
        "* `lr` is a learning rate (*α* above)\n",
        "* We will have to update `self.q[(s,a)]` for all of the data.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8FZlzaFevNkD"
      },
      "source": [
        "class TabularQ:\n",
        "    def __init__(self, states, actions):\n",
        "        self.actions = actions\n",
        "        self.states = states\n",
        "        self.q = dict([((s, a), 0.0) for s in states for a in actions])\n",
        "    def copy(self):\n",
        "        q_copy = TabularQ(self.states, self.actions)\n",
        "        q_copy.q.update(self.q)\n",
        "        return q_copy\n",
        "    def set(self, s, a, v):\n",
        "        self.q[(s,a)] = v\n",
        "    def get(self, s, a):\n",
        "        return self.q[(s,a)]\n",
        "    def update(self, data, lr):\n",
        "        # Your code here\n",
        "        pass\n",
        "        for d in data:\n",
        "            s, a, t = d[0], d[1], d[2]\n",
        "            self.set(s,a, self.get(s,a)+lr*(t-self.get(s,a)))"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aeQekM6h7vuL"
      },
      "source": [
        "## 2.2) Q_learn\n",
        "\n",
        "Complete the definition of the `Q_learn` function. It should update the entries in the `q` function, `(s, a)`, towards their estimated Q values. It should terminate after `iters` iterations and use learning rate `lr`. Use the `q.update` method, which you just wrote, to update the Q values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzaOD9VY7_4-"
      },
      "source": [
        "You will need to both simulate the agent's trajectory through the space as well as perform the updates to the Q function estimates. In this version, you should update the Q values after every transition, using a single `(s, a, t)` tuple. **The following methods and functions have already been defined for you.**\n",
        "\n",
        "\n",
        "* To start a new simulation, call `mdp.init_state()`. That will draw a state from the MDP's initial state distribution.\n",
        "* You can use the functions that we defined in HW 9: `epsilon_greedy` for action selection (epsilon_greedy takes `(q, s, eps = 0.5)` as input and returns an action) `value` takes `(q, s)` and returns the max Q value for a state.\n",
        "* To take a step in the simulation, starting in a given state, `s`, using action a, call `mdp.sim_transition(s,a)`. It will return a pair `(r, s_prime)` denoting the reward received by the agent at that step and the next state.\n",
        "* Be careful in treating terminal states. Recall that at a terminal state, there may be an immediate reward but the future expected value will be zero.\n",
        "* Return `q` so that the Tutor can test it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00YIgjEwvOab"
      },
      "source": [
        "def Q_learn(mdp, q, lr=.1, iters=100, eps = 0.5, interactive_fn=None):\n",
        "    # Your code here\n",
        "    s = mdp.init_state()\n",
        "    for i in range(iters):\n",
        "        # Your code here\n",
        "        # include this line in the iteration, where i is the iteration number\n",
        "        if interactive_fn: interactive_fn(q, i)\n",
        "        pass\n",
        "# select action\n",
        "        a = epsilon_greedy(q, s, eps=eps)\n",
        "        # execute selected action\n",
        "        r, s_prime = mdp.sim_transition(s,a)\n",
        "        if mdp.terminal(s):\n",
        "            target = r\n",
        "        else:\n",
        "            # find max q in for next state and action\n",
        "            max_q = value(q, s_prime)\n",
        "            target = r + mdp.discount_factor * max_q\n",
        "        data = [(s,a,target)]\n",
        "        q.update(data, lr)\n",
        "        s = s_prime\n",
        "    return q"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UweE9iUL897r"
      },
      "source": [
        "Run the next code blocks to test your implementation of `Q_learn`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rku6GFpdgMdH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        },
        "outputId": "e86326a9-7656-4c9f-c98d-1301c56d3dfd"
      },
      "source": [
        "MDP.TabularQ = TabularQ\n",
        "MDP.Q_learn = Q_learn\n",
        "#importlib.reload(code_for_hw10)\n",
        "\n",
        "# Test: Tabular Q-learn\n",
        "#code_for_hw10.test_learn_play(iters=100000, tabular=True, batch=False)\n",
        "test_learn_play(iters=100000, tabular=True, batch=False)\n",
        "\n",
        "# Due to newer TF version, Keras and import issues, please copy required functions from external folders to this file and rename the calling methods to resolve issues like below"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "score (0, 9.9)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'action' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-608c74f72afb>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Test: Tabular Q-learn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#code_for_hw10.test_learn_play(iters=100000, tabular=True, batch=False)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtest_learn_play\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtabular\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-15-9e8fdb0ac44f>\u001b[0m in \u001b[0;36mtest_learn_play\u001b[0;34m(d, num_layers, num_units, eps, iters, draw, tabular, batch, batch_epochs, num_episodes, episode_length)\u001b[0m\n\u001b[1;32m    458\u001b[0m                            interactive_fn=interact)\n\u001b[1;32m    459\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m         \u001b[0mqf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQ_learn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0miters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minteractive_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minteract\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    461\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'String to upload (incude quotes): \"%s\"'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mtoHex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtabular\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-26-4d4d115e05ea>\u001b[0m in \u001b[0;36mQ_learn\u001b[0;34m(mdp, q, lr, iters, eps, interactive_fn)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# select action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepsilon_greedy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0;31m# execute selected action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms_prime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmdp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msim_transition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-000bb5ff56aa>\u001b[0m in \u001b[0;36mepsilon_greedy\u001b[0;34m(q, s, eps)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;31m# Your code here (COPY FROM HW9)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'epsilon_greedy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'action' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gzEz4q7y3R8a"
      },
      "source": [
        "def tinyTerminal(s):\n",
        "    return s==4\n",
        "def tinyR(s, a):\n",
        "    if s == 1: return 1\n",
        "    elif s == 3: return 2\n",
        "    else: return 0\n",
        "def tinyTrans(s, a):\n",
        "    if s == 0:\n",
        "        if a == 'a':\n",
        "            return DDist({1 : 0.9, 2 : 0.1})\n",
        "        else:\n",
        "            return DDist({1 : 0.1, 2 : 0.9})\n",
        "    elif s == 1:\n",
        "        return DDist({1 : 0.1, 0 : 0.9})\n",
        "    elif s == 2:\n",
        "        return DDist({2 : 0.1, 3 : 0.9})\n",
        "    elif s == 3:\n",
        "        return DDist({3 : 0.1, 0 : 0.5, 4 : 0.4})\n",
        "    elif s == 4:\n",
        "        return DDist({4 : 1.0})\n",
        "\n",
        "def testQ():\n",
        "    tiny = mdp.MDP([0, 1, 2, 3, 4], ['a', 'b'], tinyTrans, tinyR, 0.9)\n",
        "    tiny.terminal = tinyTerminal\n",
        "    q = TabularQ(tiny.states, tiny.actions)\n",
        "    qf = Q_learn(tiny, q)\n",
        "    ret = list(qf.q.items())\n",
        "    expected = [((0, 'a'), 0.6649739221724159), ((0, 'b'), 0.1712369526453748),\n",
        "                ((1, 'a'), 0.7732751316011999), ((1, 'b'), 1.2034912054227331),\n",
        "                ((2, 'a'), 0.37197205380133874), ((2, 'b'), 0.45929063274463033),\n",
        "                ((3, 'a'), 1.5156163024818292), ((3, 'b'), 0.8776852768653631),\n",
        "                ((4, 'a'), 0.0), ((4, 'b'), 0.0)]\n",
        "    ok = True\n",
        "    for (s,a), v in expected:\n",
        "      qv = qf.get(s,a)\n",
        "      if abs(qv-v) > 1.0e-5:\n",
        "        print(\"Oops!  For (s=%s, a=%s) expected %s, but got %s\" % (s, a, v, qv))\n",
        "        ok = False\n",
        "    if ok:\n",
        "      print(\"Tests passed!\")\n",
        "\n",
        "random.seed(0)\n",
        "testQ()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7PXWcgn99NfK"
      },
      "source": [
        "## 2.3) Batch Q_learn\n",
        "\n",
        "Assume your previous update method has been defined.\n",
        "\n",
        "In the standard Q-learning algorithm, we make one epsilon-greedy transition based on the current Q estimate and then update the Q values. You can think of this as being like stochastic gradient descent. We can also define a version that is more like batch gradient descent, where we generate one or more \"episodes\" (sequences of transitions) using the current Q values and then update the Q values based on all the observed results. We can also keep around old transitions and use them (all or a random subset) in the update as well. **Note that as our Q value estimate evolves, the target Q value computed from a previously observed transition can change.**\n",
        "\n",
        "Implement this version of batch Q-learning that (a) generates some specifed number of episodes of a given length (see `sim_episode` below), (b) adds these to the experiences we have seen previously, and (c) updates the Q estimates based on **all the experience so far**. Return `q` so that the Tutor can test it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8WMcWSfnE-_Y"
      },
      "source": [
        "# evaluate this cell so you can use the definition in your code below\n",
        "\n",
        "def sim_episode(mdp, episode_length, policy, draw=False):\n",
        "    '''\n",
        "    Simulate an episode (sequence of transitions) of at most\n",
        "    episode_length, using policy function to select actions.  If we find\n",
        "    a terminal state, end the episode.  Return accumulated reward a list\n",
        "    of (s, a, r, s') where s' is None for transition from terminal state.\n",
        "    Also return an animation if draw=True, or None if draw=False\n",
        "    '''\n",
        "    episode = []\n",
        "    reward = 0\n",
        "    s = mdp.init_state()\n",
        "    all_states = [s]\n",
        "    for i in range(episode_length):\n",
        "        a = policy(s)\n",
        "        (r, s_prime) = mdp.sim_transition(s, a)\n",
        "        reward += r\n",
        "        if mdp.terminal(s):\n",
        "            episode.append((s, a, r, None))\n",
        "            break\n",
        "        episode.append((s, a, r, s_prime))\n",
        "        if draw:\n",
        "            mdp.draw_state(s)\n",
        "        s = s_prime\n",
        "        all_states.append(s)\n",
        "    animation = animate(all_states, mdp.n, episode_length) if draw else None\n",
        "    return reward, episode, animation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2IHoMJaI-ZhC"
      },
      "source": [
        "**Clarifications**\n",
        "\n",
        "* There should be a SINGLE call to `q.update` per iteration, not per episode or per experience. Just one call per iteration, with a lot of data.\n",
        "* Let's understand the distinction between experiences `(s, a, r, s')` and Q targets `(s, a, t)`. Note that experiences don't depend on the current estimated Q values (only on the environment we are acting in), but the \"t\" in the Q targets depends on the current Q values. So, it makes sense to store experiences across iterations, but not to store Q targets, since the Q targets change when we update our Q values. Thus, you want to continuously aggregate the experience and then, in each iteration, re-compute the Q targets under the current estimated Q values, then do the update with all of these Q targets. Here's pseudocode:\n",
        "\n",
        "\n",
        "```\n",
        "all_experiences = []\n",
        "Loop over n_iterations:\n",
        "    Loop over n_episodes:\n",
        "        Generate an episode of length episode_length, append this experience to all_experiences\n",
        "    all_q_targets = []\n",
        "    Loop over all_experiences:\n",
        "        Append Q target from one experience to all_q_targets\n",
        "        Remember to handle terminal states (where s' = None)\n",
        "    q.update(all_q_targets, lr)\n",
        "return q\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8uHjoRWPvScc"
      },
      "source": [
        "def Q_learn_batch(mdp, q, lr=.1, iters=100, eps=0.5,\n",
        "                  episode_length=10, n_episodes=2,\n",
        "                  interactive_fn=None):\n",
        "    # Your code here\n",
        "    all_experience = []\n",
        "    for i in range(iters):\n",
        "        # include this line in the iteration, where i is the iteration number\n",
        "        if interactive_fn: interactive_fn(q, i)\n",
        "        for j in range(n_episodes):\n",
        "            reward, episode, animation = sim_episode(mdp, episode_length, lambda s: epsilon_greedy(q,s,eps=eps), draw=False)\n",
        "            for e in episode:\n",
        "                all_experience.append(e)\n",
        "\n",
        "        # calculate target using [(s, a, r, s')] pairs in all_experience\n",
        "        all_q_targets = []\n",
        "        for e in all_experience:\n",
        "            if e[3] is not None:\n",
        "                t = e[2]+mdp.discount_factor * value(q, e[3])\n",
        "            else:\n",
        "                t = e[2]\n",
        "            all_q_targets.append((e[0], e[1], t))\n",
        "\n",
        "        q.update(all_q_targets, lr)\n",
        "    return q"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lv9oygbu9Sz1"
      },
      "source": [
        "Run the next code blocks to test your implementation of `Q_learn_batch`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0FnwKY5gqNI"
      },
      "source": [
        "MDP.Q_learn_batch = Q_learn_batch\n",
        "#importlib.reload(code_for_hw10)\n",
        "\n",
        "# Test: Tabular Batch Q-learn\n",
        "#code_for_hw10.test_learn_play(iters=10, tabular=True, batch=True) # Check: why do we want fewer iterations here?\n",
        "test_learn_play(iters=10, tabular=True, batch=True) # Check: why do we want fewer iterations here?"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pFvMhZuL3-Y9"
      },
      "source": [
        "def testBatchQ():\n",
        "    tiny = mdp.MDP([0, 1, 2, 3, 4], ['a', 'b'], tinyTrans, tinyR, 0.9)\n",
        "    tiny.terminal = tinyTerminal\n",
        "    q = TabularQ(tiny.states, tiny.actions)\n",
        "    qf = Q_learn_batch(tiny, q)\n",
        "    ret = list(qf.q.items())\n",
        "    expected = [((0, 'a'), 4.7566600197286535), ((0, 'b'), 3.993296047838986),\n",
        "                ((1, 'a'), 5.292467934685342), ((1, 'b'), 5.364014782870985),\n",
        "                ((2, 'a'), 4.139537149779127), ((2, 'b'), 4.155347555640753),\n",
        "                ((3, 'a'), 4.076532544818926), ((3, 'b'), 4.551442974149778),\n",
        "                ((4, 'a'), 0.0), ((4, 'b'), 0.0)]\n",
        "\n",
        "    ok = True\n",
        "    for (s,a), v in expected:\n",
        "      qv = qf.get(s,a)\n",
        "      if abs(qv-v) > 1.0e-5:\n",
        "        print(\"Oops!  For (s=%s, a=%s) expected %s, but got %s\" % (s, a, v, qv))\n",
        "        ok = False\n",
        "    if ok:\n",
        "      print(\"Tests passed!\")\n",
        "\n",
        "      return list(qf.q.items())\n",
        "\n",
        "random.seed(0)\n",
        "testBatchQ()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sj5q0rup9eq0"
      },
      "source": [
        "# 3) NN Q: Using neural networks to store the Q function\n",
        "\n",
        "We would like to operate in large or continuous state and/or action\n",
        "spaces so it is not possible (or effective) to store the $Q$ values in\n",
        "a table as we did with the <tt>TabularQ</tt> class; instead, we will\n",
        "\"store\" them by training a neural network to do regression for us,\n",
        "taking $s,a$ as input and generating (an approximation of) $Q^*(s,a)$\n",
        "as output.\n",
        "\n",
        "To train the network, we will use <i>squared Bellman error</i> as\n",
        "the loss function:\n",
        "$$\\left(\\left[R(s_t, a_t) + \\gamma \\max_{a'} Q(s_{t+1}, a'; \\theta)\\right]\n",
        "- Q(s_t, a_t;\\theta) \\right)^2$$\n",
        "where $\\theta$ stands for the current weights in the neural network\n",
        "and $Q(s, a; \\theta)$ stands for the output of the network with\n",
        "weights $\\theta$ when $(s,a)$ is the input.\n",
        "\n",
        "There are many choices of neural network architecture for storing Q\n",
        "values.  In this problem, we will:\n",
        "\n",
        "<ul>\n",
        "\n",
        "<li> Focus on the case where we have a small set of possible actions,\n",
        "so make one neural network for each possible action <math>a</math>;\n",
        "\n",
        "<li> Design that network with two <b>hidden</b> layers with ReLU units\n",
        "and a single linear output unit (although a deeper network could be\n",
        "useful); and\n",
        "\n",
        "<li> Use mean squared error (MSE) as the loss function since, we are\n",
        "predicting continuous <math>Q</math> values, which is a regression\n",
        "problem.\n",
        "\n",
        "</ul>\n",
        "\n",
        "To use a neural net to store Q values, for a given action, we will\n",
        "need to have a mapping from states to fixed-length vectors.  We will\n",
        "assume that the <code>MDP</code> class has a <code>state2vec</code>\n",
        "method that maps states to vectors.  For the simple discrete-state\n",
        "MDPs we have seen so far, this simply returns a one-hot representation\n",
        "of the state.\n",
        "\n",
        "For reference, this is our implementation of\n",
        "<code>state2vec</code> (note the shape of its returned array):\n",
        "<pre>\n",
        "    def state2vec(self, s):\n",
        "        '''\n",
        "        Return one-hot encoding of state s; used in neural network agent implementations\n",
        "        '''\n",
        "        v = np.zeros((1, len(self.states)))\n",
        "        v[0,self.states.index(s)] = 1.\n",
        "        return v\n",
        "</pre>\n",
        "\n",
        "Now, all we need to do is write a new class, called <code>NNQ</code>\n",
        "to implement neural-network version of Q-function storage; then we can\n",
        "pass an <code>NNQ</code> instance instead of a <code>TabularQ</code> instance\n",
        "into <code>Q_learn</code> or <code>Q_learn_batch</code>, and\n",
        "we will automatically have reinforcement learning with neural\n",
        "networks!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFEt8UXOR_v7"
      },
      "source": [
        "There are three methods to implement in our <code>NNQ</code>\n",
        "class. Here are some ideas for how to do that:\n",
        "<ul>\n",
        "\n",
        "<li> <code>__init__</code>: Create one neural network for each action,\n",
        "and store them in <tt>self.models</tt>. Note that <tt>actions</tt> is\n",
        "a list that may consist of integers or strings or other objects.  As a\n",
        "reminder, here's how to make a new feed-forward network using Keras:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lqcuigrnLfY0"
      },
      "source": [
        "# please evaluate this cell so you can use it in your code\n",
        "\n",
        "def make_nn(state_dim, num_hidden_layers, num_units):\n",
        "    '''\n",
        "    state_dim =\t(int) number of states\n",
        "    num_hidden_layers =\t(int) number of\tfully connected hidden layers\n",
        "    num_units =\t(int) number of\tdense relu units to use\tin hidden layers\n",
        "    '''\n",
        "    model = Sequential()\n",
        "    model.add(Dense(num_units, input_dim = state_dim, activation='relu'))\n",
        "    for i in range(num_hidden_layers-1):\n",
        "        model.add(Dense(num_units, activation='relu'))\n",
        "    model.add(Dense(1, activation='linear'))\n",
        "    model.compile(loss='mse', optimizer=Adam())\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozZOU_BnR4Tl"
      },
      "source": [
        "<ul>\n",
        "<li> <code>get(self, s, a)</code>: Use the neural network you have\n",
        "stored for action <code>a</code> to predict a Q value for state\n",
        "<code>s</code>. Feel free to consult documentation on the\n",
        "<a href=\"https://keras.io/models/model/#predict\">Keras model predict</a>\n",
        "method.</li>\n",
        "\n",
        "<li> <code>update(self, data, lr, epochs = 1)</code>: As in\n",
        "<code>TabularQ</code>, <code>data</code> is a list of <code>(s, a,t)</code>\n",
        "tuples, where <code>t</code> is a target Q value.  For each\n",
        "action <code>a</code>, you will need to:\n",
        "\n",
        "  <ul>\n",
        "  <li> Construct a training set <code>X, Y</code> of data that is\n",
        "  relevant to action <code>a</code>, where the input values are states\n",
        "  (encoded as vectors) and the output values are the target Q values;</li>\n",
        "\n",
        "  <li> Use the Keras method <code>fit(self, X, Y, epochs=epochs)</code>\n",
        "  to update the weights in th e associated network.  You can ignore\n",
        "  the <tt>lr</tt> input parameter, and let Adam in keras manage the\n",
        "  learning rate.</li>\n",
        "  </ul>\n",
        "</li>\n",
        "</ul>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKE6GBaxveRi"
      },
      "source": [
        "# Complete the following definition to implement the NNQ class\n",
        "\n",
        "class NNQ:\n",
        "    def __init__(self, states, actions, state2vec, num_layers, num_units, epochs=1):\n",
        "        self.actions = actions\n",
        "        self.states = states\n",
        "        self.epochs = epochs\n",
        "        self.state2vec = state2vec\n",
        "        state_dim = state2vec(states[0]).shape[1] # a row vector\n",
        "        self.models = {a:make_nn(state_dim, num_layers, num_units) for a in actions}\n",
        "    def get(self, s, a):\n",
        "        return self.models[a].predict(self.state2vec(s))\n",
        "    def update(self, data, lr):\n",
        "        for a in self.actions:\n",
        "            if [s for (s, at, t) in data if a==at]:\n",
        "                X = np.vstack([self.state2vec(s) for (s, at, t) in data if a==at])\n",
        "                Y = np.vstack([t for (s, at, t) in data if a==at])\n",
        "                self.models[a].fit(X, Y, epochs = self.epochs, verbose = False)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJ1PYVqe9Vq0"
      },
      "source": [
        "Run the next code blocks to test your implementation of `NNQ`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fxQqOP7Qg2-S"
      },
      "source": [
        "mdp.NNQ = NNQ\n",
        "importlib.reload(code_for_hw10)\n",
        "\n",
        "# Test: NN Q-learn\n",
        "code_for_hw10.test_learn_play(iters=100000, tabular=False, batch=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WYNYWo7BvfIC"
      },
      "source": [
        "def test_NNQ(data):\n",
        "    tiny = mdp.MDP([0, 1, 2, 3, 4], ['a', 'b'], tinyTrans, tinyR, 0.9)\n",
        "    tiny.terminal = tinyTerminal\n",
        "    q = NNQ(tiny.states, tiny.actions, tiny.state2vec, 2, 10)\n",
        "    q.update(data, 1)\n",
        "    ret =  [q.get(s,a) for s in q.states for a in q.actions]\n",
        "    expect = [np.array([[-0.07211456]]), np.array([[-0.19553234]]),\n",
        "              np.array([[-0.21926211]]), np.array([[0.01699455]]),\n",
        "              np.array([[-0.26390356]]), np.array([[0.06374809]]),\n",
        "              np.array([[0.0340214]]), np.array([[-0.18334733]]),\n",
        "              np.array([[-0.438375]]), np.array([[-0.13844737]])]\n",
        "    cnt = 0\n",
        "    ok = True\n",
        "    for s in q.states:\n",
        "      for a in q.actions:\n",
        "        if not np.all(np.abs(ret[cnt]-expect[cnt]) < 1.0e0):\n",
        "          print(\"Oops, for s=%s, a=%s expected %s but got %s\" % (s, a, expect[cnt], ret[cnt]))\n",
        "          ok = False\n",
        "        cnt += 1\n",
        "    if ok:\n",
        "      print(\"Output looks generally ok\")\n",
        "    return q\n",
        "\n",
        "test_NNQ([(0,'a',0.3),(1,'a',0.1),(0,'a',0.1),(1,'a',0.5)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "quq-Uh44_Bbo"
      },
      "source": [
        "## 3.2) Fitted Q iteration\n",
        "\n",
        "*Fitted Q iteration (FQ)* suffers less from the correlated experience problem and is generally more stable (and sometimes slower) than NNQ.\n",
        "\n",
        "FQ initializes the Q networks and an empty data set, then operates in a loop:\n",
        "\n",
        "1. Use *ϵ*-greedy exploration to generate *k* steps of experience, of the form *(s,a,r,s′)* and add them to the data set.\n",
        "2. Create one training set for each action *a*:\n",
        "\n",
        "> 1. Extract all the tuples from your data set that contain action *a*,\n",
        "> 2. Let the *X* values of your training set be all of the *s* values from your data tuples with action *a* and the *Y* values be the *r + γ max_a' Q(s', a')* values computed for each data tuple, using the Q estimates from the current network.\n",
        "\n",
        "3. Train the network for action *a* for several epochs until it has done a good job of representing this data.\n",
        "\n",
        "So, this is basically `Q_learn_batch` using `NNQ` (training with multiple epochs) to implement the Q function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0kW_URSm9mOI"
      },
      "source": [
        "Run the next code block to test your implementation of `NNQ` with batching."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SY6H72oV4z5O"
      },
      "source": [
        "# Test: NN Batch Q-learn (Fitted Q-learn)\n",
        "code_for_hw10.test_learn_play(iters=10, tabular=False, batch=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-d4QW3g0ww5H"
      },
      "source": [
        "#4) No Exit\n",
        "\n",
        "Please read the instructions in the [homework](https://lms.mitx.mit.edu/courses/course-v1:MITx+6.036+2019_Spring/courseware/Week10/week10_homework/) to set up for the game. You may use this space to display the game in Colab.\n",
        "\n",
        "For each of the learning method and Q model combinations below,\n",
        "solve the game so that it reliably gets to reward of 100 (that is, the\n",
        "learned game reliably plays 100 steps without missing the ball,\n",
        "earning a score of 100).  During learning, you should see a sequence\n",
        "of lines like: <code>score (5000, 37.5)</code>, which indicates that\n",
        "after 5000 iterations the average reward over 10 games is 37.5.  We\n",
        "are checking whether you reach a solution that gets an average reward\n",
        "100 at least one time. Try playing around with the number of\n",
        "iterations (an argument to <code>test_learn_play</code>) until you\n",
        "achieve this point. Note that we will need fewer iterations for\n",
        "Q_learn_batch, in general (check yourself: why?). After learning, the\n",
        "code prints a long \"upload string\" in HEX code.  Enter the upload\n",
        "strings in the question boxes in the homework MITx site."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zRbeO95qxJf4"
      },
      "source": [
        "# Value Iteration\n",
        "code_for_hw10.test_solve_play(draw = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zXA-F66nWizC"
      },
      "source": [
        "# Tabular Q-learn\n",
        "code_for_hw10.test_learn_play(draw=True, iters=100000, tabular=True, batch=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0xWEvfLiWmdY"
      },
      "source": [
        "# Tabular Batch Q-learn\n",
        "code_for_hw10.test_learn_play(draw=True, iters=10, tabular=True, batch=True) # Check: why do we want fewer iterations here?"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NtiEnPOZWrNa"
      },
      "source": [
        "# NN Q-learn\n",
        "code_for_hw10.test_learn_play(draw=True, iters=100000, tabular=False, batch=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GqGjYEq5WuKv"
      },
      "source": [
        "# NN Batch Q-learn (Fitted Q-learn)\n",
        "code_for_hw10.test_learn_play(draw=True, iters=10, tabular=False, batch=True)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}